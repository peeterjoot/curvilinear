%
% Copyright © 2017 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
While the reciprocal frame can be computed explcitly, it can also be computed very simply by computing the gradient of the parameters themselves.  Two theorems relate the gradient and the reciprocal frame vectors.

\maketheorem{Gradient definition of reciprocal frame vectors}{thm:curvilinearGradient:1}{

Given a curvilinear basis \( \setlr{ \Bx_k } \), the reciprocal frame vectors are

\begin{equation*}
\Bx^i = \spacegrad u_i.
\end{equation*}
} % theorem

\maketheorem{Curvilinear representation of the gradient}{thm:curvilinearGradient:2}{

Given an n-parameter representation of a vector that spans an n-dimensional space

\begin{equation*}
\Bx = \Bx(u_1, \cdots, u_n),
\end{equation*}

the curvilinear representation of the gradient is

\begin{equation*}
\spacegrad = \sum_i \Bx^i \PD{u_i}{}.
\end{equation*}

It is often convienent to write this as

\begin{equation*}
\spacegrad = \sum_{i=1}^n \Bx^i \partial_i,
\end{equation*}

or the same with sums over mixed indexes implied.

} % theorem

The proof of both are both just applications of the chain rule.  Assuming \cref{thm:curvilinearGradient:1} to be true, then the dot products of the reciprocal frame vectors with the curvilinear basis vectors are

\begin{equation}\label{eqn:curvilinearGradient:20}
\begin{aligned}
\Bx^i \cdot \Bx_j
&= (\spacegrad u_i) \cdot \PD{u_j}{\Bx} \\
&= \sum_{r,s=1}^n \lr{ \Be_r \PD{x_r}{u_i} } \cdot \lr{ \Be_s \PD{u_j}{x_s} } \\
&= \sum_{r,s=1}^n (\Be_r \cdot \Be_s) \PD{x_r}{u_i} \PD{u_j}{x_s} \\
&= \sum_{r,s=1}^n \delta_{rs} \PD{x_r}{u_i} \PD{u_j}{x_s} \\
&= \sum_{r=1}^n \PD{x_r}{u_i} \PD{u_j}{x_r} \\
&= \PD{u_i}{u_j} \\
&= \delta_{ij}.
\end{aligned}
\end{equation}

This shows that \( \Bx^i = \spacegrad u_i \) has the properties required of the reciprocal frame, proving the theorem.

The curvilinear representation of the gradient follows from the gradient representation of the reciprocal frame, and the chain rule.  The sum in \cref{thm:curvilinearGradient:2} expands as

\begin{equation}\label{eqn:curvilinearGradient:40}
\begin{aligned}
\sum_{i=1}^n
\Bx^i \PD{u_i}{F}
&=
\sum_{i=1}^n
(\spacegrad u_i) \PD{u_i}{F} \\
&=
\sum_{i,j=1}^n
\Be_j \PD{x_j}{u_i}
\PD{u_i}{F} \\
&=
\sum_{j=1}^n
\Be_j
\PD{x_j}{F} \\
&=
\spacegrad F,
\end{aligned}
\end{equation}
which proves the result.

Note that the gradient representation of the reciprocal frame is mainly useful for theoretical reasons (i.e. the proof of the curvilinear representation of the gradient).  In many cases it will likely be more difficult to compute the reciprocal frame vectors using the gradient of the parameters than

An excellent (and more detailed) discussion of the relationships of the reciprocal frame and the gradient can be found in \citep{aMacdonaldVAGC}.
